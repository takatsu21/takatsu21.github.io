<!DOCTYPE html>
<html>
   <head>
      <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
      <title>Post-processing method for improving sound quality of speech signals generated by neural vocoder</title>
      <link rel="stylesheet" type="text/css" href="style.css"/>
      <script>
         function play(path) {{
           var player = document.getElementById('player');
           player.src = path;
           player.play();
         }}
      </script>
      <style>
         .audio-cell {
         /* Center audio widgets in the table cell. */
         text-align: center;
         padding-bottom: 1px;
         padding-top: 1px;
         }
         .audio-cell-padded {
         text-align: center;
         padding-bottom: 10px;
         padding-top: 10px;
         }
         .audio-header {
         /* Don't wrap header text. */
         white-space: nowrap;
         /* Some breaking space between headers for readability. */
         padding-right: 5px;
         padding-left: 5px;
         }
         .reference-cell {
         /* For uniformity and to wrap long reference text, limit the reference cell's width. */
         width: 25%;
         padding-top: 20px;
         padding-bottom: 20px;
         }
         .sample audio {
         vertical-align: middle;
         padding-left: 3px;
         padding-right: 3px;
         }
         .round-button {
         box-sizing: border-box;
         display:block;
         width:30px;
         height:30px;
         padding-top: 8px;
         padding-left: 3px;
         line-height: 6px;
         border: 1.2px solid #000;
         border-radius: 50%;
         color: #000;
         text-align:center;
         background-color: rgba(0,0,0,0.00);
         font-size:6px;
         box-shadow: 0px 0px 2px rgba(0,0,0,1);
         transition: all 0.2s ease;
         }
         .round-button:hover {
         background-color: rgba(0,0,0,0.0);
         box-shadow: 0px 0px 4px rgba(0,0,0,1);
         }
         .round-button:active {
         background-color: rgba(0,0,0,0.01);
         box-shadow: 0px 0px 1px rgba(0,0,0,1);
         }
      </style>
   </head>
   <body>
     <div class="main">
       <article>
         <header>
            <h1>Post-processing method for improving sound quality of speech signals generated by neural vocoder</h1>
         </header>
      </article>
      <div>
        <p>
        <a>Koki Takatsu</a>,
        <a href="https://scholar.google.com/citations?user=dTORL1oAAAAJ&hl=en">Kohei Yatabe</a>,
		<a href="https://sites.google.com/site/yumakoizumiweb/profile-english">Yuma Koizumi</a>
		</p>
      </div>
      <div>
      <p><b>Abstract:</b>Neural vocoders have fundamental trade-off between the quality of generated speech signals and computational time. To overcome this trade-off, a lot of neural vocoders have been proposed in the literature. However, each neural vocoder has been developed using a technique specific to it, and hence a promising method for one neural vocoder may not be effective for another neural vocoder. Considering this situation, we propose a post-processing method that can be combined with any neural vocoder for improving the sound quality of generated speech signals. The proposed method iteratively refines speech signals so that their sound characteristics become closer to those given by the conditioning acoustic features. Our objective and subjective experiments showed that the proposed method can improve the sound quality of output signals from several different neural vocoders.<br /><br />
      <img src="fig/icassp_fig9-001.png" width="700px" class="center"/> <br />
      </p>



      <h3>Contents:</h3>
      <p>
		<a href="#DDPM">Effect on DDPM-based model output samples</a><br>
		<a href="#LJ">Effect on GAN-based model output samples of LJSpeech</a><br />
		<a href="#LT">Effect on GAN-based model output samples of LibriTTS</a><br />
      </p>


<h3 id="DDPM">Effect on DDPM-based model output samples:</h3>
<p>    
Note: We used the test signals of SpecGrad [1], WaveGrad [2], and PriorGrad [3] used in the SpecGrad paper [1]</a>.
<br /><br />
    <table rules="rows">
        <!-- <table> -->
        <tbody>
            <tr>
                <th><br></th>
                <th align="center"></th>
                <td align="center"> Example 1 <br /></td>
                <td align="center"> Example 2 <br /></td>
                <td align="center"> Example 3 <br /></td>
            </tr>
            <tr>
                <th align="center" colspan="2">Ground-truth</th>
        
                <td align="center"> <audio controls=""><source src="data/gan_gt_0.wav"></audio></td>
                <td align="center"> <audio controls=""><source src="data/gan_gt_1.wav"></audio></td>
                <td align="center"> <audio controls=""><source src="data/gan_gt_2.wav"></audio></td>
            </tr>
            <tr>
                <th rowspan="2">SpecGrad [1]</th>
                <th align="center">Generated </th>
                <td align="center"> <audio controls=""><source src="data/gan_mel_0.wav"></audio></td>
                <td align="center"> <audio controls=""><source src="data/gan_mel_1.wav"></audio></td>
                <td align="center"> <audio controls=""><source src="data/gan_mel_2.wav"></audio></td>
            </tr>
            <tr>
                <th align="center">Applied Prop. </th>
                <td align="center"> <audio controls=""><source src="data/gan_mel_0.wav"></audio></td>
                <td align="center"> <audio controls=""><source src="data/gan_mel_1.wav"></audio></td>
                <td align="center"> <audio controls=""><source src="data/gan_mel_2.wav"></audio></td>
            </tr>
            <tr>
                <th rowspan="2">WaveGrad [2]</th>
                <th align="center">Generated </th>
                <td align="center"> <audio controls=""><source src="data/gan_mel_0.wav"></audio></td>
                <td align="center"> <audio controls=""><source src="data/gan_mel_1.wav"></audio></td>
                <td align="center"> <audio controls=""><source src="data/gan_mel_2.wav"></audio></td>
            </tr>
            <tr>
                <th align="center">Applied Prop. </th>
                <td align="center"> <audio controls=""><source src="data/gan_mel_0.wav"></audio></td>
                <td align="center"> <audio controls=""><source src="data/gan_mel_1.wav"></audio></td>
                <td align="center"> <audio controls=""><source src="data/gan_mel_2.wav"></audio></td>
            </tr>
            <tr>
                <th rowspan="2">PriorGrad [3]</th>
                <th align="center">Generated </th>
                <td align="center"> <audio controls=""><source src="data/gan_mel_0.wav"></audio></td>
                <td align="center"> <audio controls=""><source src="data/gan_mel_1.wav"></audio></td>
                <td align="center"> <audio controls=""><source src="data/gan_mel_2.wav"></audio></td>
            </tr>
            <tr>
                <th align="center">Applied Prop. </th>
                <td align="center"> <audio controls=""><source src="data/gan_mel_0.wav"></audio></td>
                <td align="center"> <audio controls=""><source src="data/gan_mel_1.wav"></audio></td>
                <td align="center"> <audio controls=""><source src="data/gan_mel_2.wav"></audio></td>
            </tr>
        </table><br /><br />
        </p>




<h3 id="LJ">Effect on GAN-based model output samples of LJSpeech:</h3>
<p>
Note1: Output samples of HiFi-GAN [4], MelGAN [5],and Parallel WaveGAN [6] were downloaded from <a href="https://github.com/kan-bayashi/ParallelWaveGAN">Dr. Tomoki Hayashi's unofficial implementations</a>.
<br /><br />

Note2: The list of sample ids used in the subjective test on the LJSpeech dataset [7] is 
<a href="https://keithito.com/LJ-Speech-Dataset/">here</a>
.
<br /><br />
<table rules="rows">
    <!-- <table> -->
    <tbody>
        <tr>
            <th><br></th>
            <th align="center"></th>
            <td align="center"> Example 1 (1089_134686_000007_000004)<br /></td>
            <td align="center"> Example 2 (1089_134686_000012_000000)<br /></td>
            <td align="center"> Example 3 (121_127105_000044_000003)<br /></td>
        </tr>
        <tr>
            <th align="center" colspan="2">Ground-truth</th>
    
            <td align="center"> <audio controls=""><source src="data/gan_gt_0.wav"></audio></td>
            <td align="center"> <audio controls=""><source src="data/gan_gt_1.wav"></audio></td>
            <td align="center"> <audio controls=""><source src="data/gan_gt_2.wav"></audio></td>
        </tr>
        <tr>
            <th rowspan="2">HiFi-GAN [4]</th>
            <th align="center">Generated </th>
            <td align="center"> <audio controls=""><source src="data/gan_mel_0.wav"></audio></td>
            <td align="center"> <audio controls=""><source src="data/gan_mel_1.wav"></audio></td>
            <td align="center"> <audio controls=""><source src="data/gan_mel_2.wav"></audio></td>
        </tr>
        <tr>
            <th align="center">Applied Prop. </th>
            <td align="center"> <audio controls=""><source src="data/gan_mel_0.wav"></audio></td>
            <td align="center"> <audio controls=""><source src="data/gan_mel_1.wav"></audio></td>
            <td align="center"> <audio controls=""><source src="data/gan_mel_2.wav"></audio></td>
        </tr>
        <tr>
            <th rowspan="2">MelGAN [5]</th>
            <th align="center">Generated </th>
            <td align="center"> <audio controls=""><source src="data/gan_mel_0.wav"></audio></td>
            <td align="center"> <audio controls=""><source src="data/gan_mel_1.wav"></audio></td>
            <td align="center"> <audio controls=""><source src="data/gan_mel_2.wav"></audio></td>
        </tr>
        <tr>
            <th align="center">Applied Prop. </th>
            <td align="center"> <audio controls=""><source src="data/gan_mel_0.wav"></audio></td>
            <td align="center"> <audio controls=""><source src="data/gan_mel_1.wav"></audio></td>
            <td align="center"> <audio controls=""><source src="data/gan_mel_2.wav"></audio></td>
        </tr>
        <tr>
            <th rowspan="2">Parallel WaveGAN [6]</th>
            <th align="center">Generated </th>
            <td align="center"> <audio controls=""><source src="data/gan_mel_0.wav"></audio></td>
            <td align="center"> <audio controls=""><source src="data/gan_mel_1.wav"></audio></td>
            <td align="center"> <audio controls=""><source src="data/gan_mel_2.wav"></audio></td>
        </tr>
        <tr>
            <th align="center">Applied Prop. </th>
            <td align="center"> <audio controls=""><source src="data/gan_mel_0.wav"></audio></td>
            <td align="center"> <audio controls=""><source src="data/gan_mel_1.wav"></audio></td>
            <td align="center"> <audio controls=""><source src="data/gan_mel_2.wav"></audio></td>
        </tr>
    </table><br /><br />
    </p>






<h3 id="LT">Effect on GAN-based model output samples of LibriTTS:</h3>
<p>
Note1: Output samples of HiFi-GAN [4], MelGAN [5],and Parallel WaveGAN [6] were downloaded from <a href="https://github.com/kan-bayashi/ParallelWaveGAN">Dr. Tomoki Hayashi's unofficial implementations</a>.
<br /><br />

Note2: The list of sample ids used in the subjective test on the LibriTTS dataset [8] is 
<a href="data/libritts_test_sample_list.txt">here</a>
.
<br /><br />

<table rules="rows">
<!-- <table> -->
<tbody>
    <tr>
        <th><br></th>
        <th align="center"></th>
        <td align="center"> Example 1 (1089_134686_000007_000004)<br /></td>
        <td align="center"> Example 2 (1089_134686_000012_000000)<br /></td>
        <td align="center"> Example 3 (121_127105_000044_000003)<br /></td>
    </tr>
    <tr>
        <th align="center" colspan="2">Ground-truth</th>

        <td align="center"> <audio controls=""><source src="data/gan_gt_0.wav"></audio></td>
        <td align="center"> <audio controls=""><source src="data/gan_gt_1.wav"></audio></td>
        <td align="center"> <audio controls=""><source src="data/gan_gt_2.wav"></audio></td>
    </tr>
    <tr>
        <th rowspan="2">HiFi-GAN [4]</th>
        <th align="center">Generated </th>
        <td align="center"> <audio controls=""><source src="data/gan_mel_0.wav"></audio></td>
        <td align="center"> <audio controls=""><source src="data/gan_mel_1.wav"></audio></td>
        <td align="center"> <audio controls=""><source src="data/gan_mel_2.wav"></audio></td>
    </tr>
    <tr>
        <th align="center">Applied Prop. </th>
        <td align="center"> <audio controls=""><source src="data/gan_mel_0.wav"></audio></td>
        <td align="center"> <audio controls=""><source src="data/gan_mel_1.wav"></audio></td>
        <td align="center"> <audio controls=""><source src="data/gan_mel_2.wav"></audio></td>
    </tr>
    <tr>
        <th rowspan="2">MelGAN [5]</th>
        <th align="center">Generated </th>
        <td align="center"> <audio controls=""><source src="data/gan_mel_0.wav"></audio></td>
        <td align="center"> <audio controls=""><source src="data/gan_mel_1.wav"></audio></td>
        <td align="center"> <audio controls=""><source src="data/gan_mel_2.wav"></audio></td>
    </tr>
    <tr>
        <th align="center">Applied Prop. </th>
        <td align="center"> <audio controls=""><source src="data/gan_mel_0.wav"></audio></td>
        <td align="center"> <audio controls=""><source src="data/gan_mel_1.wav"></audio></td>
        <td align="center"> <audio controls=""><source src="data/gan_mel_2.wav"></audio></td>
    </tr>
    <tr>
        <th rowspan="2">Parallel WaveGAN [6]</th>
        <th align="center">Generated </th>
        <td align="center"> <audio controls=""><source src="data/gan_mel_0.wav"></audio></td>
        <td align="center"> <audio controls=""><source src="data/gan_mel_1.wav"></audio></td>
        <td align="center"> <audio controls=""><source src="data/gan_mel_2.wav"></audio></td>
    </tr>
    <tr>
        <th align="center">Applied Prop. </th>
        <td align="center"> <audio controls=""><source src="data/gan_mel_0.wav"></audio></td>
        <td align="center"> <audio controls=""><source src="data/gan_mel_1.wav"></audio></td>
        <td align="center"> <audio controls=""><source src="data/gan_mel_2.wav"></audio></td>
    </tr>
</table><br /><br />
</p>

This site was created following the visually design of <a href="https://google.github.io/df-conformer/wavefit/">WaveFit demo site</a>[9].


  <h3 id="references">References:</h3>
<p>
[1] Y. Koizumi, H. Zen, K. Yatabe, N. Chen, and  M. Bacchiani, “SpecGrad: Diffusion Probabilistic Model based Neural Vocoder with Adaptive Noise Spectral Shaping,”  in Proc. Interspeech, 2022. [<a href="https://arxiv.org/abs/2203.16749">paper</a>]<br/>
[2] N. Chen, Y. Zhang, H. Zen, R. J. Weiss, M. Norouzi, and W. Chan, “WaveGrad: Estimating gradients for waveform generation,” in Proc. ICLR, 2021. [<a href="https://arxiv.org/abs/2009.00713">paper</a>]<br/>
[3] S. Lee, H. Kim, C. Shin, X. Tan, C. Liu, Q. Meng, T. Qin, W. Chen, S. Yoon, and T. Y. Liu, "Efficient Neural Audio Synthesis," in Proc. ICML, 2018. [<a href="https://arxiv.org/abs/1802.08435">paper</a>]<br/>
[4] J. Kong, J. Kim, and J. Bae, "HiFi-GAN: Generative Adversarial Networks for Efficient and High Fidelity Speech Synthesis," in Proc. NeurIPS, 2020. [<a href="https://arxiv.org/abs/2010.05646">paper</a>]<br/>
[5] K. Kumar, R. Kumar, T. de Boissiere, L. Gestin, W. Z. Teoh, J. Sotelo, A. de Br&#233;bisson, Y. Bengio, and A. C. Courville, "MelGAN: Generative adversarial networks for conditional waveform synthesis," in Proc. Adv. Neural Inf. Process. Syst. NeurIPS, 2019. [<a href="https://ar5iv.labs.arxiv.org/html/1910.06711">paper</a>]<br/>
[6] R. Yamamoto, E. Song, and J.-M. Kim, "Parallel wavegan: A fast waveform generation model based on generative adversarial networks with multi-resolution spectrogram," in Proc. Int. Conf. on Acoust., Speech, and Signal Process. ICASSP, 2020. [<a href="https://ieeexplore.ieee.org/abstract/document/9053795">paper</a>]<br/>
[7] K. Ito and L. Johnson, "The lj speech dataset," 2022.[<a href="https://keithito.com/LJ-Speech-Dataset/,">site</a>]<br/>
[8] H. Zen, V. Dang, R. Clark, Y. Zhang, R. J. Weiss, Y. Jia, Z. Chen, and Y. Wu, "LibriTTS: A Corpus Derived from LibriSpeech for Text-to-Speech," in Proc. Interspeech, 2019. [<a href="https://arxiv.org/abs/1904.02882">paper</a>]<br/>
[9] Y. Koizumi, K. Yatabe, H. Zen, and M. Bacchiani, "WaveFit: An Iterative and Non-autoregressive Neural Vocoder based on Fixed-Point Iteration," in Proc. IEEE Spokeb Language Technology Workshop (SLT), 2023. [<a href="https://arxiv.org/abs/2210.01029">paper</a>]<br/>
</p>


      </div>
   </body>
</html>