<!DOCTYPE html>
<html>
   <head>
      <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
      <title>Post-processing method for improving sound quality of speech signals generated by neural vocoder</title>
      <link rel="stylesheet" type="text/css" href="style.css"/>
      <script>
         function play(path) {{
           var player = document.getElementById('player');
           player.src = path;
           player.play();
         }}
      </script>
      <style>
         .audio-cell {
         /* Center audio widgets in the table cell. */
         text-align: center;
         padding-bottom: 1px;
         padding-top: 1px;
         }
         .audio-cell-padded {
         text-align: center;
         padding-bottom: 10px;
         padding-top: 10px;
         }
         .audio-header {
         /* Don't wrap header text. */
         white-space: nowrap;
         /* Some breaking space between headers for readability. */
         padding-right: 5px;
         padding-left: 5px;
         }
         .reference-cell {
         /* For uniformity and to wrap long reference text, limit the reference cell's width. */
         width: 25%;
         padding-top: 20px;
         padding-bottom: 20px;
         }
         .sample audio {
         vertical-align: middle;
         padding-left: 3px;
         padding-right: 3px;
         }
         .round-button {
         box-sizing: border-box;
         display:block;
         width:30px;
         height:30px;
         padding-top: 8px;
         padding-left: 3px;
         line-height: 6px;
         border: 1.2px solid #000;
         border-radius: 50%;
         color: #000;
         text-align:center;
         background-color: rgba(0,0,0,0.00);
         font-size:6px;
         box-shadow: 0px 0px 2px rgba(0,0,0,1);
         transition: all 0.2s ease;
         }
         .round-button:hover {
         background-color: rgba(0,0,0,0.0);
         box-shadow: 0px 0px 4px rgba(0,0,0,1);
         }
         .round-button:active {
         background-color: rgba(0,0,0,0.01);
         box-shadow: 0px 0px 1px rgba(0,0,0,1);
         }
      </style>
   </head>
   <body>
     <div class="main">
       <article>
         <header>
            <h1>Post-processing method for improving sound quality of speech signals generated by neural vocoder</h1>
         </header>
      </article>
      <div>
        <p>
        <a>Koki Takatsu</a>,
        <a href="https://scholar.google.com/citations?user=dTORL1oAAAAJ&hl=en">Kohei Yatabe</a>,
		<a href="https://sites.google.com/site/yumakoizumiweb/profile-english">Yuma Koizumi</a>
		</p>
      </div>
      <div>
      <p><b>Abstract:</b>Neural vocoders have fundamental trade-off between the quality of generated speech signals and computational time. To overcome this trade-off, a lot of neural vocoders have been proposed in the literature. However, each neural vocoder has been developed using a technique specific to it, and hence a promising method for one neural vocoder may not be effective for another neural vocoder. Considering this situation, we propose a post-processing method that can be combined with any neural vocoder for improving the sound quality of generated speech signals. The proposed method iteratively refines speech signals so that their sound characteristics become closer to those given by the conditioning acoustic features. Our objective and subjective experiments showed that the proposed method can improve the sound quality of output signals from several different neural vocoders.<br /><br />
      <img src="fig/icassp_fig9-001.png" width="700px" class="center"/> <br />
      </p>



      <h3>Contents:</h3>
      <p>
		<a href="#intermediate">Intermediate outputs examples of a WaveFit-5 model</a><br>
		<a href="#ddpm">Comparison with DDPM-based models and WaveRNN</a><br />
		<a href="#gan">Comparison with GAN-based models on LibriTTS</a><br />
      </p>


<h3 id="intermediate">Intermediate outputs examples of a WaveFit-5 model:</h3>
<p>
<table>
<tbody>
    <tr>
        <th align="center"></th>
        <td align="center"> Example 1<br /></td>
        <td align="center"> Example 2<br /></td>
        <td align="center"> Example 3<br /></td>
        <td align="center"> Example 4<br /></td>
    </tr>
    <tr>
        <th align="center">Ground-truth</th>
        <td align="center"> <audio controls=""><source src="data/wf5_gt_0.wav"></audio></td>
        <td align="center"> <audio controls=""><source src="data/wf5_gt_1.wav"></audio></td>
        <td align="center"> <audio controls=""><source src="data/wf5_gt_2.wav"></audio></td>
        <td align="center"> <audio controls=""><source src="data/wf5_gt_3.wav"></audio></td>
    </tr>
    <tr>
        <th align="center">Iteration 1</th>
        <td align="center"> <audio controls=""><source src="data/wf5_iter1_0.wav"></audio></td>
        <td align="center"> <audio controls=""><source src="data/wf5_iter1_1.wav"></audio></td>
        <td align="center"> <audio controls=""><source src="data/wf5_iter1_2.wav"></audio></td>
        <td align="center"> <audio controls=""><source src="data/wf5_iter1_3.wav"></audio></td>
    </tr>
    <tr>
        <th align="center">Iteration 2</th>
        <td align="center"> <audio controls=""><source src="data/wf5_iter2_0.wav"></audio></td>
        <td align="center"> <audio controls=""><source src="data/wf5_iter2_1.wav"></audio></td>
        <td align="center"> <audio controls=""><source src="data/wf5_iter2_2.wav"></audio></td>
        <td align="center"> <audio controls=""><source src="data/wf5_iter2_3.wav"></audio></td>
    </tr>
    <tr>
        <th align="center">Iteration 3</th>
        <td align="center"> <audio controls=""><source src="data/wf5_iter3_0.wav"></audio></td>
        <td align="center"> <audio controls=""><source src="data/wf5_iter3_1.wav"></audio></td>
        <td align="center"> <audio controls=""><source src="data/wf5_iter3_2.wav"></audio></td>
        <td align="center"> <audio controls=""><source src="data/wf5_iter3_3.wav"></audio></td>
    </tr>
    <tr>
        <th align="center">Iteration 4</th>
        <td align="center"> <audio controls=""><source src="data/wf5_iter4_0.wav"></audio></td>
        <td align="center"> <audio controls=""><source src="data/wf5_iter4_1.wav"></audio></td>
        <td align="center"> <audio controls=""><source src="data/wf5_iter4_2.wav"></audio></td>
        <td align="center"> <audio controls=""><source src="data/wf5_iter4_3.wav"></audio></td>
    </tr>
    <tr>
        <th align="center">Final output</th>
        <td align="center"> <audio controls=""><source src="data/wf5_iter5_0.wav"></audio></td>
        <td align="center"> <audio controls=""><source src="data/wf5_iter5_1.wav"></audio></td>
        <td align="center"> <audio controls=""><source src="data/wf5_iter5_2.wav"></audio></td>
        <td align="center"> <audio controls=""><source src="data/wf5_iter5_3.wav"></audio></td>
    </tr>
</tbody>
</table><br /><br />
</p>






<h3 id="ddpm">Comparison with GAN-based models on LJSpeech:</h3>
<p>
Note1: Output samples of HiFi-GAN [4], MelGAN [5],and Parallel WaveGAN [6] were downloaded from <a href="https://github.com/kan-bayashi/ParallelWaveGAN">Dr. Tomoki Hayashi's unofficial implementations</a>.
<br /><br />

Note2: The list of sample ids used in the subjective test on the LJSpeech dataset [7] is 
<a href="https://keithito.com/LJ-Speech-Dataset/">here</a>
.
<br /><br />
<table>
<tbody>
    <tr>
        <th align="center"></th>
        <td align="center"> Example 1<br /></td>
        <td align="center"> Example 2<br /></td>
        <td align="center"> Example 3<br /></td>
        <td align="center"> Example 4<br /></td>
    </tr>
    <tr>
        <th align="center">Ground-truth</th>
        <td align="center"> <audio controls=""><source src="data/ddpm_gt_0.wav"></audio></td>
        <td align="center"> <audio controls=""><source src="data/ddpm_gt_1.wav"></audio></td>
        <td align="center"> <audio controls=""><source src="data/ddpm_gt_2.wav"></audio></td>
        <td align="center"> <audio controls=""><source src="data/ddpm_gt_3.wav"></audio></td>
    </tr>
    <tr>
        <th align="center">WaveRNN [3]</th>
        <td align="center"> <audio controls=""><source src="data/ddpm_wrnn_0.wav"></audio></td>
        <td align="center"> <audio controls=""><source src="data/ddpm_wrnn_1.wav"></audio></td>
        <td align="center"> <audio controls=""><source src="data/ddpm_wrnn_2.wav"></audio></td>
        <td align="center"> <audio controls=""><source src="data/ddpm_wrnn_3.wav"></audio></td>
    </tr>
    <tr>
        <th align="center">SpecGrad-3 [1]</th>
        <td align="center"> <audio controls=""><source src="data/ddpm_sg_0.wav"></audio></td>
        <td align="center"> <audio controls=""><source src="data/ddpm_sg_1.wav"></audio></td>
        <td align="center"> <audio controls=""><source src="data/ddpm_sg_2.wav"></audio></td>
        <td align="center"> <audio controls=""><source src="data/ddpm_sg_3.wav"></audio></td>
    </tr>
    <tr>
        <th align="center">InferGrad-3 [2]</th>
        <td align="center"> <audio controls=""><source src="data/ddpm_ig_0.wav"></audio></td>
        <td align="center"> <audio controls=""><source src="data/ddpm_ig_1.wav"></audio></td>
        <td align="center"> <audio controls=""><source src="data/ddpm_ig_2.wav"></audio></td>
        <td align="center"> <audio controls=""><source src="data/ddpm_ig_3.wav"></audio></td>
    </tr>
    <tr>
        <th align="center">WaveFit-3</th>
        <td align="center"> <audio controls=""><source src="data/ddpm_wf3_0.wav"></audio></td>
        <td align="center"> <audio controls=""><source src="data/ddpm_wf3_1.wav"></audio></td>
        <td align="center"> <audio controls=""><source src="data/ddpm_wf3_2.wav"></audio></td>
        <td align="center"> <audio controls=""><source src="data/ddpm_wf3_3.wav"></audio></td>
    </tr>
    <tr>
        <th align="center">WaveFit-5</th>
        <td align="center"> <audio controls=""><source src="data/ddpm_wf5_0.wav"></audio></td>
        <td align="center"> <audio controls=""><source src="data/ddpm_wf5_1.wav"></audio></td>
        <td align="center"> <audio controls=""><source src="data/ddpm_wf5_2.wav"></audio></td>
        <td align="center"> <audio controls=""><source src="data/ddpm_wf5_3.wav"></audio></td>
    </tr>
</tbody>
</table><br /><br />
</p>







<h3 id="gan">Comparison with GAN-based models on LibriTTS:</h3>
<p>
Note1: Output samples of HiFi-GAN [4], MelGAN [5],and Parallel WaveGAN [6] were downloaded from <a href="https://github.com/kan-bayashi/ParallelWaveGAN">Dr. Tomoki Hayashi's unofficial implementations</a>.
<br /><br />

Note2: The list of sample ids used in the subjective test on the LibriTTS dataset [8] is 
<a href="data/libritts_test_sample_list.txt">here</a>
.
<br /><br />

<table rules="rows">
<!-- <table> -->
<tbody>
    <tr>
        <th><br></th>
        <th align="center"></th>
        <td align="center"> Example 1 (1089_134686_000007_000004)<br /></td>
        <td align="center"> Example 2 (1089_134686_000012_000000)<br /></td>
        <td align="center"> Example 3 (121_127105_000044_000003)<br /></td>
        <td align="center"> Example 4 (1284_1180_000024_000001)<br /></td>
    </tr>
    <tr>
        <th align="center" colspan="2">Ground-truth</th>

        <td align="center"> <audio controls=""><source src="data/gan_gt_0.wav"></audio></td>
        <td align="center"> <audio controls=""><source src="data/gan_gt_1.wav"></audio></td>
        <td align="center"> <audio controls=""><source src="data/gan_gt_2.wav"></audio></td>
        <td align="center"> <audio controls=""><source src="data/gan_gt_3.wav"></audio></td>
    </tr>
    <tr>
        <th rowspan="2">HiFi-GAN [4]</th>
        <th align="center">Generated </th>
        <td align="center"> <audio controls=""><source src="data/gan_mel_0.wav"></audio></td>
        <td align="center"> <audio controls=""><source src="data/gan_mel_1.wav"></audio></td>
        <td align="center"> <audio controls=""><source src="data/gan_mel_2.wav"></audio></td>
        <td align="center"> <audio controls=""><source src="data/gan_mel_3.wav"></audio></td>
    </tr>
    <tr>
        <th align="center">Applied Prop. </th>
        <td align="center"> <audio controls=""><source src="data/gan_mel_0.wav"></audio></td>
        <td align="center"> <audio controls=""><source src="data/gan_mel_1.wav"></audio></td>
        <td align="center"> <audio controls=""><source src="data/gan_mel_2.wav"></audio></td>
        <td align="center"> <audio controls=""><source src="data/gan_mel_3.wav"></audio></td>
    </tr>
    <tr>
        <th rowspan="2">MelGAN [5]</th>
        <th align="center">Generated </th>
        <td align="center"> <audio controls=""><source src="data/gan_mel_0.wav"></audio></td>
        <td align="center"> <audio controls=""><source src="data/gan_mel_1.wav"></audio></td>
        <td align="center"> <audio controls=""><source src="data/gan_mel_2.wav"></audio></td>
        <td align="center"> <audio controls=""><source src="data/gan_mel_3.wav"></audio></td>
    </tr>
    <tr>
        <th align="center">Applied Prop. </th>
        <td align="center"> <audio controls=""><source src="data/gan_mel_0.wav"></audio></td>
        <td align="center"> <audio controls=""><source src="data/gan_mel_1.wav"></audio></td>
        <td align="center"> <audio controls=""><source src="data/gan_mel_2.wav"></audio></td>
        <td align="center"> <audio controls=""><source src="data/gan_mel_3.wav"></audio></td>
    </tr>
    <tr>
        <th rowspan="2">Parallel WaveGAN [6]</th>
        <th align="center">Generated </th>
        <td align="center"> <audio controls=""><source src="data/gan_mel_0.wav"></audio></td>
        <td align="center"> <audio controls=""><source src="data/gan_mel_1.wav"></audio></td>
        <td align="center"> <audio controls=""><source src="data/gan_mel_2.wav"></audio></td>
        <td align="center"> <audio controls=""><source src="data/gan_mel_3.wav"></audio></td>
    </tr>
    <tr>
        <th align="center">Applied Prop. </th>
        <td align="center"> <audio controls=""><source src="data/gan_mel_0.wav"></audio></td>
        <td align="center"> <audio controls=""><source src="data/gan_mel_1.wav"></audio></td>
        <td align="center"> <audio controls=""><source src="data/gan_mel_2.wav"></audio></td>
        <td align="center"> <audio controls=""><source src="data/gan_mel_3.wav"></audio></td>
    </tr>
</table><br /><br />
</p>



  <h3 id="references">References:</h3>
<p>
[1] Y. Koizumi, H. Zen, K. Yatabe, N. Chen, and  M. Bacchiani, “SpecGrad: Diffusion Probabilistic Model based Neural Vocoder with Adaptive Noise Spectral Shaping,”  in Proc. Interspeech, 2022. [<a href="https://arxiv.org/abs/2203.16749">paper</a>]<br/>
[2] Z. Chen, X. Tan, K. Wang, S. Pan, D. Mandic, L. He, and S. Zhao, “InferGrad: Improving Diffusion Models for Vocoder by Considering Inference in Training,” in Proc. ICASSP, 2022. [<a href="https://arxiv.org/abs/2202.03751">paper</a>]<br/>
[3] N. Kalchbrenner, W. Elsen, K. Simonyan, S. Noury, N. Casagrande, W. Lockhart, F. Stimberg, A. van den Oord, S. Dieleman, and K. Kavukcuoglu, "Efficient Neural Audio Synthesis," in Proc. ICML, 2018. [<a href="https://arxiv.org/abs/1802.08435">paper</a>]<br/>
[4] J. Kong, J. Kim, and J. Bae, "HiFi-GAN: Generative Adversarial Networks for Efficient and High Fidelity Speech Synthesis," in Proc. NeurIPS, 2020. [<a href="https://arxiv.org/abs/2010.05646">paper</a>]<br/>
[5] K. Kumar, R. Kumar, T. de Boissiere, L. Gestin, W. Z. Teoh, J. Sotelo, A. de Br&#233;bisson, Y. Bengio, and A. C. Courville, "MelGAN: Generative adversarial networks for conditional waveform synthesis," in Proc. Adv. Neural Inf. Process. Syst. NeurIPS, 2019. [<a href="https://ar5iv.labs.arxiv.org/html/1910.06711">paper</a>]<br/>
[6] R. Yamamoto, E. Song, and J.-M. Kim, "Parallel wavegan: A fast waveform generation model based on generative adversarial networks with multi-resolution spectrogram," in Proc. Int. Conf. on Acoust., Speech, and Signal Process. ICASSP, 2020. [<a href="https://ieeexplore.ieee.org/abstract/document/9053795">paper</a>]<br/>
[7] K. Ito and L. Johnson, "The lj speech dataset," 2022.[<a href="https://keithito.com/LJ-Speech-Dataset/,">cite</a>]<br/>
[8] H. Zen, V. Dang, R. Clark, Y. Zhang, R. J. Weiss, Y. Jia, Z. Chen, and Y. Wu, "LibriTTS: A Corpus Derived from LibriSpeech for Text-to-Speech," in Proc. Interspeech, 2019. [<a href="https://arxiv.org/abs/1904.02882">paper</a>]<br/>
</p>


      </div>
   </body>
</html>