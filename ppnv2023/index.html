<!DOCTYPE html>
<html>
   <head>
      <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
      <title>Post-processing method for improving sound quality of speech signals generated by neural vocoder</title>
      <link rel="stylesheet" type="text/css" href="style.css"/>
      <script>
         function play(path) {{
           var player = document.getElementById('player');
           player.src = path;
           player.play();
         }}
      </script>
      <style>
         .audio-cell {
         /* Center audio widgets in the table cell. */
         text-align: center;
         padding-bottom: 1px;
         padding-top: 1px;
         }
         .audio-cell-padded {
         text-align: center;
         padding-bottom: 10px;
         padding-top: 10px;
         }
         .audio-header {
         /* Don't wrap header text. */
         white-space: nowrap;
         /* Some breaking space between headers for readability. */
         padding-right: 5px;
         padding-left: 5px;
         }
         .reference-cell {
         /* For uniformity and to wrap long reference text, limit the reference cell's width. */
         width: 25%;
         padding-top: 20px;
         padding-bottom: 20px;
         }
         .sample audio {
         vertical-align: middle;
         padding-left: 3px;
         padding-right: 3px;
         }
         .round-button {
         box-sizing: border-box;
         display:block;
         width:30px;
         height:30px;
         padding-top: 8px;
         padding-left: 3px;
         line-height: 6px;
         border: 1.2px solid #000;
         border-radius: 50%;
         color: #000;
         text-align:center;
         background-color: rgba(0,0,0,0.00);
         font-size:6px;
         box-shadow: 0px 0px 2px rgba(0,0,0,1);
         transition: all 0.2s ease;
         }
         .round-button:hover {
         background-color: rgba(0,0,0,0.0);
         box-shadow: 0px 0px 4px rgba(0,0,0,1);
         }
         .round-button:active {
         background-color: rgba(0,0,0,0.01);
         box-shadow: 0px 0px 1px rgba(0,0,0,1);
         }
      </style>
   </head>
   <body>
     <div class="main">
       <article>
         <header>
            <h1>Post-processing method for improving sound quality of speech signals generated by neural vocoder</h1>
         </header>
      </article>
      <div>
        <p>
        <a>Koki Takatsu</a>,
        <a href="https://scholar.google.com/citations?user=dTORL1oAAAAJ&hl=en">Kohei Yatabe</a>,
		<a href="https://sites.google.com/site/yumakoizumiweb/profile-english">Yuma Koizumi</a>
		</p>
      </div>
      <div>
      <p><b>Abstract:</b>Neural vocoders have fundamental trade-off between the sound quality of generated speech signals and their computational time. To overcome this trade-off, a lot of neural vocoders have been proposed in the literature. However, each neural vocoder has been developed based on a technique specific to it, and hence a promising method for one neural vocoder may not be effective for another neural vocoder. Considering this situation, we propose a post-processing method that can be combined with any neural vocoder for improving the sound quality of generated speech signals. The proposed method iteratively refines speech signals so that their sound characteristics become closer to those given by the conditioning acoustic features. Our objective and subjective experiments showed that the proposed method can improve the sound quality of speech signals generated by several different neural vocoders.<br /><br />
      <img src="fig/icassp_fig17-001.png" width="1000px" class="center"/> <br />
      </p>



      <h3>Contents:</h3>
      <p>
		<a href="#DDPM">Effect on DDPM-based model output samples</a><br>
		<a href="#LJ">Effect on GAN-based model output samples of LJSpeech</a><br />
		<a href="#LT">Effect on GAN-based model output samples of LibriTTS</a><br />
        <a href="#VIS">Visual observation of the effect of the proposed method</a><br />
      </p>


<h3 id="DDPM">Effect on DDPM-based model output samples:</h3>
<p>    
Note: We used the test signals of SpecGrad [1], WaveGrad [2], and PriorGrad [3] used in the SpecGrad paper [1]</a>.
<br /><br />
    <table rules="rows">
        <!-- <table> -->
        <tbody>
            <tr>
                <th><br></th>
                <th align="center"></th>
                <td align="center"> Example 1 <br /></td>
                <td align="center"> Example 2 <br /></td>
                <td align="center"> Example 3 <br /></td>
            </tr>
            <tr>
                <th align="center" colspan="2">Ground-truth</th>
        
                <td align="center"> <audio controls=""><source src="wav/SG/GT/10062418368059573713.wav"></audio></td>
                <td align="center"> <audio controls=""><source src="wav/SG/GT/10153835546511903137.wav"></audio></td>
                <td align="center"> <audio controls=""><source src="wav/SG/GT/10273503412906970013.wav"></audio></td>
            </tr>
            <tr>
                <th rowspan="2">SpecGrad [1]</th>
                <th align="center">Generated </th>
                <td align="center"> <audio controls=""><source src="wav/SG/before/10062418368059573713.wav"></audio></td>
                <td align="center"> <audio controls=""><source src="wav/SG/before/10153835546511903137.wav"></audio></td>
                <td align="center"> <audio controls=""><source src="wav/SG/before/10273503412906970013.wav"></audio></td>
            </tr>
            <tr>
                <th align="center">Applied Prop. </th>
                <td align="center"> <audio controls=""><source src="wav/SG/after/10062418368059573713.wav"></audio></td>
                <td align="center"> <audio controls=""><source src="wav/SG/after/10153835546511903137.wav"></audio></td>
                <td align="center"> <audio controls=""><source src="wav/SG/after/10273503412906970013.wav"></audio></td>
            </tr>
            <tr>
                <th rowspan="2">WaveGrad [2]</th>
                <th align="center">Generated </th>
                <td align="center"> <audio controls=""><source src="wav/WG/before/10062418368059573713.wav"></audio></td>
                <td align="center"> <audio controls=""><source src="wav/WG/before/10153835546511903137.wav"></audio></td>
                <td align="center"> <audio controls=""><source src="wav/WG/before/10273503412906970013.wav"></audio></td>
            </tr>
            <tr>
                <th align="center">Applied Prop. </th>
                <td align="center"> <audio controls=""><source src="wav/WG/after/10062418368059573713.wav"></audio></td>
                <td align="center"> <audio controls=""><source src="wav/WG/after/10153835546511903137.wav"></audio></td>
                <td align="center"> <audio controls=""><source src="wav/WG/after/10273503412906970013.wav"></audio></td>
            </tr>
            <tr>
                <th rowspan="2">PriorGrad [3]</th>
                <th align="center">Generated </th>
                <td align="center"> <audio controls=""><source src="wav/PG/before/10062418368059573713.wav"></audio></td>
                <td align="center"> <audio controls=""><source src="wav/PG/before/10153835546511903137.wav"></audio></td>
                <td align="center"> <audio controls=""><source src="wav/PG/before/10273503412906970013.wav"></audio></td>
            </tr>
            <tr>
                <th align="center">Applied Prop. </th>
                <td align="center"> <audio controls=""><source src="wav/PG/after/10062418368059573713.wav"></audio></td>
                <td align="center"> <audio controls=""><source src="wav/PG/after/10153835546511903137.wav"></audio></td>
                <td align="center"> <audio controls=""><source src="wav/PG/after/10273503412906970013.wav"></audio></td>
            </tr>
        </table><br /><br />
        </p>




<h3 id="LJ">Effect on GAN-based model output samples of LJSpeech:</h3>
<p>
Note1: Output samples of HiFi-GAN [4], MelGAN [5],and Parallel WaveGAN [6] were downloaded from <a href="https://github.com/kan-bayashi/ParallelWaveGAN">Dr. Tomoki Hayashi's unofficial implementations</a>.
<br /><br />

Note2: The list of output sample's names used in the subjective test on the LJSpeech dataset [7] is 
<a href="wav/LJ_namelist.txt">here</a>
.
<br /><br />
<table rules="rows">
    <!-- <table> -->
    <tbody>
        <tr>
            <th><br></th>
            <th align="center"></th>
            <td align="center"> Example 1 (LJ050-0100)<br /></td>
            <td align="center"> Example 2 (LJ050-0173)<br /></td>
            <td align="center"> Example 3 (LJ050-0245)<br /></td>
        </tr>
        <tr>
            <th align="center" colspan="2">Ground-truth</th>
    
            <td align="center"> <audio controls=""><source src="wav/LJ_HF/GT/LJ050-0100_gen.wav"></audio></td>
            <td align="center"> <audio controls=""><source src="wav/LJ_HF/GT/LJ050-0173_gen.wav"></audio></td>
            <td align="center"> <audio controls=""><source src="wav/LJ_HF/GT/LJ050-0245_gen.wav"></audio></td>
        </tr>
        <tr>
            <th rowspan="2">HiFi-GAN [4]</th>
            <th align="center">Generated </th>
            <td align="center"> <audio controls=""><source src="wav/LJ_HF/before/LJ050-0100_gen.wav"></audio></td>
            <td align="center"> <audio controls=""><source src="wav/LJ_HF/before/LJ050-0173_gen.wav"></audio></td>
            <td align="center"> <audio controls=""><source src="wav/LJ_HF/before/LJ050-0245_gen.wav"></audio></td>
        </tr>
        <tr>
            <th align="center">Applied Prop. </th>
            <td align="center"> <audio controls=""><source src="wav/LJ_HF/after/LJ050-0100_gen.wav"></audio></td>
            <td align="center"> <audio controls=""><source src="wav/LJ_HF/after/LJ050-0173_gen.wav"></audio></td>
            <td align="center"> <audio controls=""><source src="wav/LJ_HF/after/LJ050-0245_gen.wav"></audio></td>
        </tr>
        <tr>
            <th rowspan="2">MelGAN [5]</th>
            <th align="center">Generated </th>
            <td align="center"> <audio controls=""><source src="wav/LJ_M/before/LJ050-0100_gen.wav"></audio></td>
            <td align="center"> <audio controls=""><source src="wav/LJ_M/before/LJ050-0173_gen.wav"></audio></td>
            <td align="center"> <audio controls=""><source src="wav/LJ_M/before/LJ050-0245_gen.wav"></audio></td>
        </tr>
        <tr>
            <th align="center">Applied Prop. </th>
            <td align="center"> <audio controls=""><source src="wav/LJ_M/after/LJ050-0100_gen.wav"></audio></td>
            <td align="center"> <audio controls=""><source src="wav/LJ_M/after/LJ050-0173_gen.wav"></audio></td>
            <td align="center"> <audio controls=""><source src="wav/LJ_M/after/LJ050-0245_gen.wav"></audio></td>
        </tr>
        <tr>
            <th rowspan="2">Parallel WaveGAN [6]</th>
            <th align="center">Generated </th>
            <td align="center"> <audio controls=""><source src="wav/LJ_PW/before/LJ050-0100_gen.wav"></audio></td>
            <td align="center"> <audio controls=""><source src="wav/LJ_PW/before/LJ050-0173_gen.wav"></audio></td>
            <td align="center"> <audio controls=""><source src="wav/LJ_PW/before/LJ050-0245_gen.wav"></audio></td>
        </tr>
        <tr>
            <th align="center">Applied Prop. </th>
            <td align="center"> <audio controls=""><source src="wav/LJ_PW/after/LJ050-0100_gen.wav"></audio></td>
            <td align="center"> <audio controls=""><source src="wav/LJ_PW/after/LJ050-0173_gen.wav"></audio></td>
            <td align="center"> <audio controls=""><source src="wav/LJ_PW/after/LJ050-0245_gen.wav"></audio></td>
        </tr>
    </table><br /><br />
    </p>






<h3 id="LT">Effect on GAN-based model output samples of LibriTTS:</h3>
<p>
Note1: Output samples of HiFi-GAN [4], MelGAN [5],and Parallel WaveGAN [6] were downloaded from <a href="https://github.com/kan-bayashi/ParallelWaveGAN">Dr. Tomoki Hayashi's unofficial implementations</a>.
<br /><br />

Note2: The list of output sample's names used in the subjective test on the LibriTTS dataset [8] is 
<a href="wav/LibriTTS_namelist.txt">here</a>
.
<br /><br />

<table rules="rows">
<!-- <table> -->
<tbody>
    <tr>
        <th><br></th>
        <th align="center"></th>
        <td align="center"> Example 1 (1188_133604_000011_000003)<br /></td>
        <td align="center"> Example 2 (1089_123859_000017_000001)<br /></td>
        <td align="center"> Example 3 (121_127105_000043_000002)<br /></td>
    </tr>
    <tr>
        <th align="center" colspan="2">Ground-truth</th>

        <td align="center"> <audio controls=""><source src="wav/Libri_HF/GT/1188_133604_000011_000003_gen.wav"></audio></td>
        <td align="center"> <audio controls=""><source src="wav/Libri_HF/GT/121_123859_000017_000001_gen.wav"></audio></td>
        <td align="center"> <audio controls=""><source src="wav/Libri_HF/GT/121_127105_000043_000002_gen.wav"></audio></td>
    </tr>
    <tr>
        <th rowspan="2">HiFi-GAN [4]</th>
        <th align="center">Generated </th>
        <td align="center"> <audio controls=""><source src="wav/Libri_HF/before/1188_133604_000011_000003_gen.wav"></audio></td>
        <td align="center"> <audio controls=""><source src="wav/Libri_HF/before/121_123859_000017_000001_gen.wav"></audio></td>
        <td align="center"> <audio controls=""><source src="wav/Libri_HF/before/121_127105_000043_000002_gen.wav"></audio></td>
    </tr>
    <tr>
        <th align="center">Applied Prop. </th>
        <td align="center"> <audio controls=""><source src="wav/Libri_HF/after/1188_133604_000011_000003_gen.wav"></audio></td>
        <td align="center"> <audio controls=""><source src="wav/Libri_HF/after/121_123859_000017_000001_gen.wav"></audio></td>
        <td align="center"> <audio controls=""><source src="wav/Libri_HF/after/121_127105_000043_000002_gen.wav"></audio></td>
    </tr>
    <tr>
        <th rowspan="2">MelGAN [5]</th>
        <th align="center">Generated </th>
        <td align="center"> <audio controls=""><source src="wav/Libri_M/before/1188_133604_000011_000003_gen.wav"></audio></td>
        <td align="center"> <audio controls=""><source src="wav/Libri_M/before/121_123859_000017_000001_gen.wav"></audio></td>
        <td align="center"> <audio controls=""><source src="wav/Libri_M/before/121_127105_000043_000002_gen.wav"></audio></td>
    </tr>
    <tr>
        <th align="center">Applied Prop. </th>
        <td align="center"> <audio controls=""><source src="wav/Libri_M/after/1188_133604_000011_000003_gen.wav"></audio></td>
        <td align="center"> <audio controls=""><source src="wav/Libri_M/after/121_123859_000017_000001_gen.wav"></audio></td>
        <td align="center"> <audio controls=""><source src="wav/Libri_M/after/121_127105_000043_000002_gen.wav"></audio></td>
    </tr>
    <tr>
        <th rowspan="2">Parallel WaveGAN [6]</th>
        <th align="center">Generated </th>
        <td align="center"> <audio controls=""><source src="wav/Libri_PW/before/1188_133604_000011_000003_gen.wav"></audio></td>
        <td align="center"> <audio controls=""><source src="wav/Libri_PW/before/121_123859_000017_000001_gen.wav"></audio></td>
        <td align="center"> <audio controls=""><source src="wav/Libri_PW/before/121_127105_000043_000002_gen.wav"></audio></td>
    </tr>
    <tr>
        <th align="center">Applied Prop. </th>
        <td align="center"> <audio controls=""><source src="wav/Libri_PW/after/1188_133604_000011_000003_gen.wav"></audio></td>
        <td align="center"> <audio controls=""><source src="wav/Libri_PW/after/121_123859_000017_000001_gen.wav"></audio></td>
        <td align="center"> <audio controls=""><source src="wav/Libri_PW/after/121_127105_000043_000002_gen.wav"></audio></td>
    </tr>
</table><br /><br />
</p>

<h3 id="VIS">Visual observation of the effect of the proposed method:</h3>
<p>
    The effect of the proposed method on the speech signal can be visually observed using log-mel spectrograms</a>.
    <br /><br />
    <div style="text-align:center;">
        </div>
        <p style="text-align:center;"><b>Ground-truth</b></p>
    <img src="fig/MELGT.png" width="500px" class="center">
    <br/>
    <audio controls="" class="center"><source src="wav/WG/before/10062418368059573713.wav"></audio>
    <br/>
<p>
    The log-mel spectrograms of the speech signal before and after applying the proposed method are as follows: 
    <table align="center">
        <td><div style="text-align:center;">
        </div>
        <p style="text-align:center;"><b>Generated speech signal by WaveGrad[2]</b></p>
        <img src="fig/MELGEN.png" width="500px" class="center">
        <audio controls="" class="center"><source src="wav/WG/before/10062418368059573713.wav"></audio>
        </td>
        <td><div style="text-align:center;">
        </div>
        <p style="text-align:center;"><b>Signal applied Proposed method</b></p>
        <img src="fig/MELPROP.png" width="500px" class="center">
        <audio controls="" class="center"><source src="wav/WG/before/10062418368059573713.wav"></audio>
        </td>
    </table>
<b><i>This site was created following the visually design of</i> <a href="https://google.github.io/df-conformer/wavefit/">WaveFit demo site</a> [9].</b>


  <h3 id="references">References:</h3>
<p>
[1] Y. Koizumi, H. Zen, K. Yatabe, N. Chen, and  M. Bacchiani, “SpecGrad: Diffusion Probabilistic Model based Neural Vocoder with Adaptive Noise Spectral Shaping,”  in Proc. Interspeech, 2022. [<a href="https://arxiv.org/abs/2203.16749">paper</a>]<br/>
[2] N. Chen, Y. Zhang, H. Zen, R. J. Weiss, M. Norouzi, and W. Chan, “WaveGrad: Estimating Gradients for Waveform Generation,” in Proc. ICLR, 2021. [<a href="https://arxiv.org/abs/2009.00713">paper</a>]<br/>
[3] S. Lee, H. Kim, C. Shin, X. Tan, C. Liu, Q. Meng, T. Qin, W. Chen, S. Yoon, and T. Y. Liu, "PriorGrad: Improving Conditional Denoising Diffusion Models with Data-Dependent Adaptive Prior," in Proc. ICLR, 2022. [<a href="https://arxiv.org/abs/2106.06406">paper</a>]<br/>
[4] J. Kong, J. Kim, and J. Bae, "HiFi-GAN: Generative Adversarial Networks for Efficient and High Fidelity Speech Synthesis," in Proc. NeurIPS, 2020. [<a href="https://arxiv.org/abs/2010.05646">paper</a>]<br/>
[5] K. Kumar, R. Kumar, T. de Boissiere, L. Gestin, W. Z. Teoh, J. Sotelo, A. de Br&#233;bisson, Y. Bengio, and A. C. Courville, "MelGAN: Generative Adversarial Networks for Conditional Waveform Synthesis," in Proc. NeurIPS, 2019. [<a href="https://ar5iv.labs.arxiv.org/html/1910.06711">paper</a>]<br/>
[6] R. Yamamoto, E. Song, and J.-M. Kim, "Parallel WaveGAN: A Fast Waveform Generation Model based on Generative Adversarial Networks with Multi-Resolution Spectrogram," in Proc. ICASSP, 2020. [<a href="https://ieeexplore.ieee.org/abstract/document/9053795">paper</a>]<br/>
[7] K. Ito and L. Johnson, "The LJ Speech Dataset," 2022.[<a href="https://keithito.com/LJ-Speech-Dataset/">site</a>]<br/>
[8] H. Zen, V. Dang, R. Clark, Y. Zhang, R. J. Weiss, Y. Jia, Z. Chen, and Y. Wu, "LibriTTS: A Corpus Derived from LibriSpeech for Text-to-Speech," in Proc. Interspeech, 2019. [<a href="https://arxiv.org/abs/1904.02882">paper</a>]<br/>
[9] Y. Koizumi, K. Yatabe, H. Zen, and M. Bacchiani, "WaveFit: An Iterative and Non-Autoregressive Neural Vocoder Based on Fixed-Point Iteration," in Proc. SLT, 2023. [<a href="https://arxiv.org/abs/2210.01029">paper</a>]<br/>
</p>


      </div>
   </body>
</html>